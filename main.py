# -*- coding: utf-8 -*-
"""Term Paper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dyzw37KpRTWwtGPbdE-g8eunmlucbKrB
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
#import libraries
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, auc
from sklearn import metrics
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation,Dropout
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import SGD,Adam
import keras
from keras.layers.convolutional import Conv1D
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import Flatten

col_list = ["sport","dport",
            "pkts","bytes",
            "state","dur",
            "mean","stddev",
            "sum","min",
            "max","spkts",
            "dpkts","sbytes",
            "dbytes","rate",
            "srate","drate",
            "attack"
           ]

df_num=pd.read_csv('drive/MyDrive/data/DDoS_TCP.csv',sep=';',usecols=col_list)#nrows=50000,

"""**Data Preparation**"""

from sklearn.preprocessing import LabelEncoder
from sklearn import preprocessing, metrics
cat_features = ['state']
encoder = LabelEncoder()
label_encoder = preprocessing.LabelEncoder()
for feature in cat_features:
    encoded = label_encoder.fit_transform(df_num[cat_features])
    df_num['state'] = encoded

df_num.dtypes

print(df_num.isna().sum().sum())
df_num=df_num.dropna()
print(df_num.isna().sum().sum())

df_num.sample(5)

plt.figure(figsize=(10,10))
data_corr=df_num.corr()
sns.heatmap(data_corr, cmap = 'coolwarm')

"""**Distribution Classes**"""

df_num.groupby('attack').size()

"""**Remove Useless Features**"""

for col in (df_num.iloc[:,:-1].columns):
    if(df_num[col].min()==df_num[col].max()):
            df_num.drop(col, axis=1, inplace=True)

"""**Feature Scaling**"""

# Normalization OR Standardization
def standardize(df,col):
    df[col]= (df[col]-df[col].mean())/(df[col].std()) # Standardization
    #df[col]= (df[col]-df[col].min())/(df[col].max()-df[col].min()) #Normalization

for i in (df_num.iloc[:,:-1].columns):
    standardize (df_num,i)

df_num.head()

target=['attack']
features = [c for c in df_num.columns if c!="attack"]

#from sklearn.feature_selection import SelectKBest, f_classif
#X_train_new = SelectKBest(f_classif, k=12).fit_transform(df_num[features], df_num[target])

"""**Split Dataset**"""

X = df_num[features].values # Features
y = df_num[target].values # Target

X=X.astype(np.float32)
y=y.astype(np.float32)

X.shape,y.shape,

del df_num

import gc
gc.collect()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)

X_train.shape,X_test.shape,y_train.shape,y_test.shape

#X_train.shape,X_val.shape,X_test.shape,y_train.shape,y_val.shape,y_test.shape
X_train_ann = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))
X_test_ann = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],1))
print(X_train_ann.shape)
print(X_test_ann.shape)

"""**Build ANN Model**"""

learning_rate=0.001
batch_size=5000
epochs = 10

model_save = ModelCheckpoint('./DDoS_TCP.h5',
                             save_best_only = True,
                             save_weights_only = False,
                             monitor = 'val_loss',
                             mode = 'min', verbose = 1)
early_stop = EarlyStopping(monitor = 'val_loss', min_delta = 0.001,
                           patience = 50, mode = 'min', verbose = 1,
                           restore_best_weights = True)
reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.75,
                              patience = 10, min_delta = 0.001,
                              mode = 'min', verbose = 1)

# ANN
def Create_Model_ANN(num_columns, num_labels,learning_rate):
    model = Sequential()
    model.add(Dense(64,input_dim=num_columns, activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(256, activation='relu'))
    model.add(Dense(512, activation='relu'))
    model.add(Dense(256, activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(num_labels,activation='sigmoid'))

    adam = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
    #adam = Adam(lr=learning_rate)

    model.compile(loss='binary_crossentropy', optimizer=adam,metrics=[keras.metrics.AUC(name='auc')])

    return model

model = Create_Model_ANN(X_test.shape[1], 1,learning_rate)

history=model.fit(X_train_ann,
          y_train,
          validation_data=(X_test_ann,y_test),
          callbacks = [model_save, early_stop, reduce_lr],
          verbose=1,
          batch_size=batch_size,
          epochs=epochs)

"""**Evaluation**"""

y_pred = model.predict(X_test)
AUC = metrics.roc_auc_score(y_test,y_pred)
print("AUC: {:.3f}".format(AUC))

hist_df = pd.DataFrame(history.history)
hist_df.to_csv('history.csv')

"""**Training Curves**"""

plt.figure(figsize=(10,5))
plt.plot(range(history.epoch[-1]+1),history.history['val_auc'],label='val_auc')
plt.plot(range(history.epoch[-1]+1),history.history['auc'],label='auc')
plt.title('auc'); plt.xlabel('Epoch'); plt.ylabel('auc');plt.legend();
plt.show()

plt.figure(figsize=(10,5))
plt.plot(range(history.epoch[-1]+1),history.history['val_loss'],label='Val_loss')
plt.plot(range(history.epoch[-1]+1),history.history['loss'],label='loss')
plt.title('loss'); plt.xlabel('Epoch'); plt.ylabel('loss');plt.legend();
plt.show()

"""**ROC Curve**"""

def generate_results(y_test, y_score):
    # print(y_score)
    fpr, tpr, _ = roc_curve(y_test, y_score)
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange',
             lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    plt.show()

generate_results(y_test, y_pred)